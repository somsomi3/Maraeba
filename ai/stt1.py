import librosa
import torch
import numpy as np
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from scipy.spatial.distance import euclidean
from dtw import dtw

# Wav2Vec2 ëª¨ë¸ ë¡œë“œ (ìŒì†Œ ë²¡í„° ì¶”ì¶œìš©)
model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-english"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

def extract_phoneme_vectors(file_path):
    """ ìŒì†Œ(Phoneme) ë‹¨ìœ„ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ """
    audio, rate = librosa.load(file_path, sr=16000)
    audio, _ = librosa.effects.trim(audio)  # ğŸ”¹ ì¹¨ë¬µ ì œê±°

    input_values = processor(audio, return_tensors="pt", sampling_rate=16000).input_values.to("cuda" if torch.cuda.is_available() else "cpu")
    
    with torch.no_grad():
        logits = model(input_values).logits  # ëª¨ë¸ ì¶œë ¥ (CTC ì˜ˆì¸¡ê°’)

    # ğŸ”¹ Softmax ì ìš© ì œê±° (logits ì§ì ‘ ì‚¬ìš©)
    phoneme_vectors = logits.squeeze().cpu().numpy()

    return phoneme_vectors

# ë‘ ê°œì˜ ìŒì†Œ ë²¡í„° ì¶”ì¶œ
phoneme1 = extract_phoneme_vectors("audio/ì•„1.wav")
phoneme2 = extract_phoneme_vectors("audio/ì´.wav")

# âœ… DTWë¥¼ ì‚¬ìš©í•˜ì—¬ ë°œìŒ ìœ ì‚¬ë„ ê³„ì‚° (ìŒì†Œ ë²¡í„° ê¸°ë°˜ ë¹„êµ)
alignment = dtw(phoneme1, phoneme2, dist_method=euclidean)
dtw_dist = alignment.distance / alignment.index1.shape[0]  # ê¸¸ì´ ì •ê·œí™”

print(dtw_dist)
# âœ… DTW ê±°ë¦¬ â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (í¼ì„¼íŠ¸ ê°’)
MAX_DTW_DIST = 100  # ğŸ”¹ DTW ê±°ë¦¬ ê¸°ì¤€ ìµœëŒ€ê°’ ì¡°ì •
# dtw_similarity = max(0, 100 - (dtw_dist / MAX_DTW_DIST) * 100)

# print(f"ğŸ”¹ ë°œìŒ ìœ ì‚¬ë„ (DTW ê±°ë¦¬ ê¸°ë°˜ - ìŒì†Œ í™•ë¥  ë²¡í„°): {dtw_similarity:.2f}%")
