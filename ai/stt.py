import speech_recognition as sr
from pydub import AudioSegment
import librosa
import torch
from app import processor, model

# í•œ ê¸€ìëŠ” ì¸ì‹ì´ ì•ˆë˜ì–´ì„œ ì´ì–´ë¶™ì´ê¸°
def duplicate_and_merge_audio(input_audio, silence_duration=300):
    sound = AudioSegment.from_wav(input_audio)
    silence = AudioSegment.silent(duration=silence_duration)

    # ì›ë³¸ + ë³µì œë³¸ ì´ì–´ë¶™ì´ê¸°
    combined = sound + silence + sound  

    combined.export(input_audio, format="wav")
    return

def recognize_speech_from_file(audio_file_path):
    recognizer = sr.Recognizer()
    
    duplicate_and_merge_audio(audio_file_path)

    try:
        with sr.AudioFile(audio_file_path) as source:
            audio = recognizer.record(source)

        # Google STT
        google_text = recognizer.recognize_google(audio, language="ko-KR")
        google_result = "".join(google_text.split())  # ëª¨ë“  ê³µë°± ì œê±°
        text = google_result
    except sr.UnknownValueError:
        text = "ì˜¤ë””ì˜¤ë¥¼ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except sr.RequestError as e:
        text = f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}"

        # # Whisper STT
        # try:
        #     whisper_model = whisper.load_model("large", device="cpu")  # ì‘ì€ ëª¨ë¸ ì‚¬ìš©
        #     whisper_result = whisper_model.transcribe(audio_file_path, language="korean", task="transcribe", temperature=0)  # í•œê¸€ë§Œ ë³€í™˜, ì¼ê´€ëœ ê²°ê³¼ ìœ ì§€
        #     text = "".join(whisper_result["text"].split())  # ëª¨ë“  ê³µë°± ì œê±°
        #     print(text)
        #     results["Whisper STT"] = text

        # except Exception as e:
        #     results["Whisper STT"] = f"Whisper ì˜¤ë¥˜: {e}"

    except Exception as e:
        text = f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"

    return text

def recognize_en(audio_path):
    text = transcribe_audio(audio_path)
    return text


def transcribe_audio(file_path):
    """ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
    audio, rate = librosa.load(file_path, sr=16000)
    audio, _ = librosa.effects.trim(audio)  # ğŸ”¹ ì¹¨ë¬µ ì œê±°
    input_values = processor(audio, return_tensors="pt", sampling_rate=16000).input_values.to("cuda" if torch.cuda.is_available() else "cpu")
    
    with torch.no_grad():
        logits = model(input_values).logits

    # ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸ ë³€í™˜
    prediction = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(prediction)[0]

    return transcription.strip()
