import librosa
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

# Wav2Vec2 ëª¨ë¸ ë¡œë“œ
model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-english"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

def transcribe_audio(file_path):
    """ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
    audio, rate = librosa.load(file_path, sr=16000)
    audio, _ = librosa.effects.trim(audio)  # ğŸ”¹ ì¹¨ë¬µ ì œê±°
    input_values = processor(audio, return_tensors="pt", sampling_rate=16000).input_values.to("cuda" if torch.cuda.is_available() else "cpu")
    
    with torch.no_grad():
        logits = model(input_values).logits

    # ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸ ë³€í™˜
    prediction = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(prediction)[0]

    return transcription.strip()

# ë‘ ê°œì˜ ìŒì„± íŒŒì¼ì„ ë³€í™˜
text1 = transcribe_audio("audio/ì•„1.wav")
text2 = transcribe_audio("audio/ì´.wav")

# ê²°ê³¼ ì¶œë ¥
print(f"ğŸ”¹ ë³€í™˜ëœ í…ìŠ¤íŠ¸ 1: {text1}")
print(f"ğŸ”¹ ë³€í™˜ëœ í…ìŠ¤íŠ¸ 2: {text2}")
